# Classification_Performance_Threshold
Contains the related notebook that explains how classification models can be tested for performance. LinkedIn article: https://www.linkedin.com/pulse/i-can-barely-recall-enough-precision-little-what-anurag-halder/

I find it very difficult and unfair to remember jargon, anything that forces me to memorize generally fails me. To avoid ambiguity and overcome the challenge, I tend to, as much possible, logically try to arrive at concepts. At most times these jargon have very simple ideas and are christened for reference. One such case I feel are terms: Recall, Precision, Specificity and Sensitivity.

My target in this article would be to try and explain from my view of looking at the problem and solving for:

Binary Classification Model Performance

Arriving at Optimal Threshold values for Classification

Thereby understanding what is ROC (Receiver Operator Curve) and arriving at AUC (Area Under Curve)

It will not cover the concepts of unbalanced classes etc, but once corrected for, these steps are all equally applicable
